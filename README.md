### High-Level Summary

This project is a complete Retrieval-Augmented Generation (RAG) pipeline. It takes PDF documents you provide, processes them into a searchable knowledge base, and then uses that knowledge base to answer your questions. It leverages large language models (LLMs) for both understanding the documents and generating answers.

---

### Detailed File-by-File Breakdown

#### 1. `create_database.py`

This script is responsible for building the vector database from your source documents.

*   **`load_documents()`**: This function finds and loads all PDF files located in the `data/test_files` directory using a `DirectoryLoader` from LangChain.
*   **`split_text(documents)`**: It takes the loaded documents and splits them into smaller, more manageable chunks of text (300 characters each with a 100-character overlap). This is crucial for the retrieval process to find very specific pieces of information.
*   **`save_to_chroma(chunks)`**: This is the final step in the creation process.
    1.  It first deletes any existing database in the `chroma` directory to ensure a fresh start.
    2.  It then uses the `BAAI/bge-base-en-v1.5` embedding model via `TogetherEmbeddings` to convert each text chunk into a numerical representation (a vector).
    3.  These vectors are then stored in a [ChromaDB](https://www.trychroma.com/) vector database, which is saved to the `chroma` directory.
*   **`main()`**: This function orchestrates the entire process by calling the above functions in order.

#### 2. `retrieval.py`

This script is the user-facing part of the application, used to ask questions and get answers.

*   **`load_database()`**: It loads the ChromaDB database that was previously created and saved in the `chroma` directory.
*   **`create_retriever()`**: It creates a "retriever" object from the loaded database. This object is designed to find and return the most relevant document chunks based on a query.
*   **`main()`**: This is the core of the retrieval process:
    1.  It prompts you to enter a question.
    2.  It uses the retriever to search the ChromaDB for chunks of text that are semantically similar to your question.
    3.  It combines the content of these retrieved chunks into a single block of text called `context`.
    4.  It then connects to the TogetherAI API and passes your original question along with the retrieved `context` to the `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` language model.
    5.  Finally, it prints the answer generated by the model and also lists the "Sources" (the metadata of the specific chunks) that were used to generate the answer, which is great for verification.

#### 3. `testing_functions/view_database.py`

This is a utility script for inspecting the contents of your ChromaDB database to ensure everything was created correctly.

*   **`load_database()`**: Loads the ChromaDB from the `chroma` directory.
*   **`get_all_documents(collection)`**: Fetches every single entry from the database.
*   **`view_database_info()`**: This function calls other helper functions to display:
    *   **`print_database_stats()`**: Total number of chunks in the database.
    *   **`print_source_files()`**: A list of the unique PDF files that the chunks came from.
    *   **`print_sample_documents()`**: A preview of the first few chunks in the database, showing their content and metadata (like which file they came from).

***

### End-to-End Workflow

1.  **Ingestion**: You place PDF files in the `data/test_files` folder.
2.  **Processing**: You run `create_database.py`. It splits the PDFs into chunks, creates vector embeddings from them using a model on TogetherAI, and saves them to a local ChromaDB database.
3.  **Retrieval & Synthesis**: You run `retrieval.py` and ask a question. The script finds the most relevant chunks from your documents, provides them as context to a powerful language model on TogetherAI, and returns a synthesized answer based on your documents.
4.  **Verification**: You can use `testing_functions/view_database.py` to check the state and contents of your database.

#### API KEYS

Make sure to upload your API keys in a `.env` for:

`OPENAI_API_KEY` and `TOGETHER_API_KEY`