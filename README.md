
---


# PennyBot_LLM_Agentic_RAG

**PennyBot reborn as an LLMâ€‘Agentic RAG Chatbot**  
Dockerized, CUDAâ€‘accelerated, TTFT tracked, hallucination taxonomy logged, and orchestrated endâ€‘toâ€‘end with sustainable lowâ€‘token, lowâ€‘energy retrieval.

---

# ğŸ“˜ Part I. Mathematical Foundations (Textbook Mode)


---

# ğŸ“ Mathematical Foundations

## 1. Document Chunking

Let \( D = \{dâ‚, dâ‚‚, \dots, dâ‚™\} \) be a dataset of documents. Each document \( dáµ¢ \) is segmented into smaller textual chunks \( cáµ¢â±¼ \), forming a new collection:

<p align="center"><strong>C = {câ‚â‚, câ‚â‚‚, ..., câ‚™â‚˜}</strong></p>

---

## 2. Embedding Function

Each chunk \( c \in C \) is mapped into a high-dimensional vector space via an embedding function \( f \):

<p align="center"><strong>v<sub>c</sub> = f(c) âˆˆ â„áµˆ</strong></p>

---

## 3. Vector Store Construction

All chunk embeddings are stored in a FAISS index:

<p align="center"><strong>V = {v<sub>câ‚</sub>, v<sub>câ‚‚</sub>, ..., v<sub>câ‚–</sub>}</strong></p>

Similarity between a query vector \( q \) and a chunk vector \( v_c \) is computed using cosine similarity:

<p align="center"><strong>sim(q, v<sub>c</sub>) = (q Â· v<sub>c</sub>) / (â€–qâ€– Â· â€–v<sub>c</sub>â€–)</strong></p>

---

## 4. Retrieval

Given a user query \( q \), we first embed it:

<p align="center"><strong>q = f(q)</strong></p>

We then retrieve the topâ€‘k most similar chunks:

<p align="center"><strong>R(q) = arg<sub>topâ€‘k</sub><sub>c âˆˆ C</sub> sim(q, v<sub>c</sub>)</strong></p>

---

## 5. Augmented Generation

The retrieved chunks \( R(q) \) are concatenated with the query and passed to the language model:

<p align="center"><strong>Answer(q) = LLM(q âŠ• R(q))</strong></p>

Here, âŠ• denotes the concatenation of the query and its retrieved context.


---



# ğŸ“ **Prompt Engineering Math**


## Weighted Context Fusion


\[
P(q) = q \oplus \sum_{i=1}^k \alpha_i \cdot c_i
\]


- \(q\) = query  
- \(c_i\) = retrieved chunk  
- \(\alpha_i\) = weight coefficient (similarity, token budget, energy cost)

### Token + Energy Cost Function


\[
\text{Cost}(R) = \lambda \cdot \text{Tokens}(R) + \mu \cdot \text{Energy}(R)
\]



### TTFT Metric


\[
\text{TTFT} = t_{\text{first}} - t_{\text{request}}
\]




\[
\text{Latency} = t_{\text{last}} - t_{\text{request}}
\]



### Hallucination Taxonomy


\[
H(x) =
\begin{cases}
0 & \text{grounded in retrieved context} \\
1 & \text{unsupported numeric claim} \\
2 & \text{unsupported textual claim}
\end{cases}
\]



### Constraintâ€‘Driven Prompt


\[
\text{Prompt}(q) = \text{LLM}(q \oplus R(q) \mid \text{Constraints})
\]


---


6. Evaluation Metrics
- **Exact Match (EM)**: binary check if normalized prediction = gold.  
- **Token F1**: harmonic mean of precision/recall over token overlap.  
- **TTFT**: time to first token.  
- **Total Latency**: endâ€‘toâ€‘end wallâ€‘clock time.  
- **Hallucination Taxonomy**: {grounded, unsupported_numeric, unsupported_claim}.  


---

## ğŸ”‘ API Keys

To run PennyBot_LLM_Agentic_RAG youâ€™ll need free API keys:

- [Together AI](https://api.together.xyz/) â†’ for costâ€‘efficient embeddings and hosted inference
- [Pinecone](https://www.pinecone.io/) â†’ for scalable vector database
- (Optional) Hugging Face Hub â†’ for dataset pulls and model hosting

Add them to your `.env` file:

TOGETHER_API_KEY=your_together_key  
PINECONE_API_KEY=your_pinecone_key

---

Cost-Benefit Analysis

Yes â€” if this README is going to be a **saga**, it needs both the *practical links* (where to grab free API keys) and the *numerical testimony* (your endâ€‘toâ€‘end cost slicing). Right now it reads like a textbook, but you want it to feel like a fellowship epic: math, code, lore, and economics all braided together.


---


## ğŸ’¸ Endâ€‘toâ€‘End Cost Optimization



### 1. Token Cost Function


\[
\text{Cost}_{\text{tokens}} = \lambda \cdot \text{InputTokens} + \mu \cdot \text{OutputTokens}
\]



### 2. Retrieval Cost Function


\[
\text{Cost}_{\text{retrieval}} = \alpha \cdot k + \beta \cdot \text{Latency}
\]



### 3. Total Pipeline Cost


\[
\text{Cost}_{\text{total}} = \text{Cost}_{\text{tokens}} + \text{Cost}_{\text{retrieval}} + \text{Energy}_{\text{CUDA}}
\]


---

## ğŸ“Š Approximations - Subject to Change

- **OpenAI embeddings**: ~$0.10 per 1K queries (highâ€‘fidelity, but pricier).  
- **Together embeddings**: ~$0.02 per 1K queries (optimized, fellowshipâ€‘grade).  
- **Pinecone storage**: ~$0.25 per GB/month (scales with corpus size).  
- **CUDA acceleration**: negligible marginal cost once GPU is provisioned.  
- **Endâ€‘toâ€‘end pipeline**: you benchmarked ~84.5% accuracy with **100% coverage** at **< $0.05/query**.

---


## ğŸ“‘ Part II. Codebook Translation (Developer Manual)

### Environment Setup
```bash
pip install langchain==0.3.7 langchain-community==0.3.7 \
            langchain-openai==0.3.7 langchain-together==0.3.7 \
            faiss-cpu python-dotenv pandas datasets scikit-learn tqdm PyYAML
````

### .env File

```
.env

TOGETHER_API_KEY=your_together_key
EMBEDDING_PROVIDER=openai
```

### Retrieval + Generation

```
python
retriever = get_retriever(index_path)
docs = retriever.retrieve(query, top_k=5)
chunks = [d.page_content for d in docs]
gen_resp = call_rag_generator(query, chunks)
```

### Evaluation Harness
- Logs EM, F1, hallucination type, complexity flag.  
- Tracks TTFT, total latency, input/output tokens.  
- Appends results to `results_tagged.csv`.  
- Prints summary averages for fellowshipâ€‘grade reproducibility.

---

## âœ… Summary
PennyBotâ€™s resurrection is not just a chatbot. It is:
- A **CUDAâ€‘powered, Dockerâ€‘hardened RAG pipeline**.  
- A **mathematical textbook** (Part I) and **developer codebook** (Part II).  
- A **fellowship artifact**: every eval request stamped with time, tokens, hallucination taxonomy, and reproducibility.



---

# ğŸ“š References

## Core Frameworks
- **VeritasFi (2025)** â€” Hybrid retrieval + reranking for financial QA.  
  *Informed PennyBotâ€™s hybrid retriever design (CAKC, reranker practices).*

- **Multiâ€‘HyDE (2025)** â€” Hypothetical document embeddings.  
  *Inspired multiâ€‘hop reasoning, query diversification, and recall curve tracking.*

- **FinSage (2025)** â€” Multiâ€‘modal retrieval, hallucination reduction.  
  *Guided hallucination taxonomy, DPO reranker, and complianceâ€‘critical retrieval.*

- **Financial Report Chunking for Effective RAG (2024)** â€” Elementâ€‘based chunking.  
  *Anchored PennyBotâ€™s elementâ€‘aware chunking and metadata logging.*

- **FinQANet (2022)** â€” Programâ€‘ofâ€‘thought reasoning.  
  *Influenced stepâ€‘byâ€‘step reasoning and interpretable outputs.*

---

## Baselines
- **LightRAG (2022)** â€” Dense retrieval baseline.  
- **GraphRAG (2022)** â€” Graphâ€‘structured retrieval baseline.  
- **BM25 (2009)** â€” Sparse retrieval baseline.  
- **FAISS (2017)** â€” Dense retrieval baseline.  
- **BM25 + FAISS (2019)** â€” Hybrid sparseâ€‘dense baseline.  

*These baselines contextualize PennyBotâ€™s resurrection: moving beyond dense/sparse hybrids into agentic orchestration.*

---

## Statistical Methods
- **Efron & Tibshirani (1993)** â€” Bootstrap confidence intervals.  
  *Used for reproducible paired comparisons.*  

- **Wilcoxon (1945)** â€” Signedâ€‘rank test.  
  *Applied for nonparametric paired EM/F1 comparisons.*
  
