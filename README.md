
---


PennyBot_LLM_Agentic_RAG

**PennyBot reborn as an LLMâ€‘Agentic RAG Chatbot**  
Dockerized, CUDAâ€‘accelerated, TTFT tracked, hallucination taxonomy logged, and orchestrated endâ€‘toâ€‘end with sustainable lowâ€‘token, lowâ€‘energy retrieval.

---

ğŸ“‚ Repository Structure

AgenticRAG/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_all.bat
â”œâ”€â”€ run_log.txt
â”œâ”€â”€ all_questions_tagged.csv
â”œâ”€â”€ financebench_open_source.jsonl
â”œâ”€â”€ build_index.py          # Vector store construction
â”œâ”€â”€ chat_cli.py             # Command-line chatbot interface
â”œâ”€â”€ etl.py                  # Extract-transform-load pipeline
â”œâ”€â”€ evaluate.py             # Evaluation harness (EM, F1, TTFT, hallucination taxonomy)
â”œâ”€â”€ generate_corpus.py      # Corpus generation scripts
â”œâ”€â”€ ingest_and_filter.py    # Ingestion + filtering logic
â”œâ”€â”€ pinecone_rest.py        # Pinecone API wrapper
â”œâ”€â”€ rag_agent_library.py    # Core agent orchestration library
â”œâ”€â”€ seed_from_jsonl.py      # Seed corpus from JSONL
â”œâ”€â”€ __pycache__/            # Python cache (ignored in .gitignore)
â””â”€â”€ .vscode/                # VS Code settings (ignored in .gitignore)

---

ğŸ“˜ Part I. Mathematical Foundations (Textbook Mode)


```markdown
### 1. Document Representation


\[
D = \{d_1, d_2, \dots, d_n\}
\]


Each document \(d_i\) is segmented into smaller textual chunks:


\[
C = \{c_{11}, c_{12}, \dots, c_{nm}\}
\]



### 2. Embedding Function


\[
v_c = f(c) \in \mathbb{R}^d
\]



### 3. Vector Store Construction


\[
V = \{v_{c_1}, v_{c_2}, \dots, v_{c_k}\}
\]




\[
\text{sim}(q, v_c) = \frac{q \cdot v_c}{\|q\| \cdot \|v_c\|}
\]



### 4. Retrieval


\[
q = f(q)
\]




\[
R(q) = \text{arg topâ€‘k}_{c \in C} \ \text{sim}(q, v_c)
\]



### 5. Augmented Generation


\[
\text{Answer}(q) = \text{LLM}(q \oplus R(q))
\]


Here, \(\oplus\) denotes concatenation of query and retrieved context.
```
---


6. Evaluation Metrics
- **Exact Match (EM)**: binary check if normalized prediction = gold.  
- **Token F1**: harmonic mean of precision/recall over token overlap.  
- **TTFT**: time to first token.  
- **Total Latency**: endâ€‘toâ€‘end wallâ€‘clock time.  
- **Hallucination Taxonomy**: {grounded, unsupported_numeric, unsupported_claim}.  


ğŸ“ Prompt Engineering Math

```
### Weighted Context Fusion


\[
P(q) = q \oplus \sum_{i=1}^k \alpha_i \cdot c_i
\]


- \(q\) = query  
- \(c_i\) = retrieved chunk  
- \(\alpha_i\) = weight coefficient (similarity, token budget, energy cost)

### Token + Energy Cost Function


\[
\text{Cost}(R) = \lambda \cdot \text{Tokens}(R) + \mu \cdot \text{Energy}(R)
\]



### TTFT Metric


\[
\text{TTFT} = t_{\text{first}} - t_{\text{request}}
\]




\[
\text{Latency} = t_{\text{last}} - t_{\text{request}}
\]



### Hallucination Taxonomy


\[
H(x) =
\begin{cases}
0 & \text{grounded in retrieved context} \\
1 & \text{unsupported numeric claim} \\
2 & \text{unsupported textual claim}
\end{cases}
\]



### Constraintâ€‘Driven Prompt


\[
\text{Prompt}(q) = \text{LLM}(q \oplus R(q) \mid \text{Constraints})
\]

```

---

## ğŸ”‘ API Keys

To run PennyBot_LLM_Agentic_RAG youâ€™ll need free API keys:

- [Together AI](https://api.together.xyz/) â†’ for costâ€‘efficient embeddings and hosted inference
- [Pinecone](https://www.pinecone.io/) â†’ for scalable vector database
- (Optional) Hugging Face Hub â†’ for dataset pulls and model hosting

Add them to your `.env` file:

TOGETHER_API_KEY=your_together_key  
PINECONE_API_KEY=your_pinecone_key

---

Cost-Benefit Analysis

Yes â€” if this README is going to be a **saga**, it needs both the *practical links* (where to grab free API keys) and the *numerical testimony* (your endâ€‘toâ€‘end cost slicing). Right now it reads like a textbook, but you want it to feel like a fellowship epic: math, code, lore, and economics all braided together.


---


## ğŸ’¸ Endâ€‘toâ€‘End Cost Optimization

```

### 1. Token Cost Function


\[
\text{Cost}_{\text{tokens}} = \lambda \cdot \text{InputTokens} + \mu \cdot \text{OutputTokens}
\]



### 2. Retrieval Cost Function


\[
\text{Cost}_{\text{retrieval}} = \alpha \cdot k + \beta \cdot \text{Latency}
\]



### 3. Total Pipeline Cost


\[
\text{Cost}_{\text{total}} = \text{Cost}_{\text{tokens}} + \text{Cost}_{\text{retrieval}} + \text{Energy}_{\text{CUDA}}
\]


```

---

## ğŸ“Š Approximations - Subject to Change

- **OpenAI embeddings**: ~$0.10 per 1K queries (highâ€‘fidelity, but pricier).  
- **Together embeddings**: ~$0.02 per 1K queries (optimized, fellowshipâ€‘grade).  
- **Pinecone storage**: ~$0.25 per GB/month (scales with corpus size).  
- **CUDA acceleration**: negligible marginal cost once GPU is provisioned.  
- **Endâ€‘toâ€‘end pipeline**: you benchmarked ~84.5% accuracy with **100% coverage** at **< $0.05/query**.

---


## ğŸ“‘ Part II. Codebook Translation (Developer Manual)

### Environment Setup
```bash
pip install langchain==0.3.7 langchain-community==0.3.7 \
            langchain-openai==0.3.7 langchain-together==0.3.7 \
            faiss-cpu python-dotenv pandas datasets scikit-learn tqdm PyYAML
````

### .env File

```
.env

TOGETHER_API_KEY=your_together_key
EMBEDDING_PROVIDER=openai
```

### Retrieval + Generation

```
python
retriever = get_retriever(index_path)
docs = retriever.retrieve(query, top_k=5)
chunks = [d.page_content for d in docs]
gen_resp = call_rag_generator(query, chunks)
```

### Evaluation Harness
- Logs EM, F1, hallucination type, complexity flag.  
- Tracks TTFT, total latency, input/output tokens.  
- Appends results to `results_tagged.csv`.  
- Prints summary averages for fellowshipâ€‘grade reproducibility.

---

## âœ… Summary
PennyBotâ€™s resurrection is not just a chatbot. It is:
- A **CUDAâ€‘powered, Dockerâ€‘hardened RAG pipeline**.  
- A **mathematical textbook** (Part I) and **developer codebook** (Part II).  
- A **fellowship artifact**: every eval request stamped with time, tokens, hallucination taxonomy, and reproducibility.



---

# ğŸ“š References

## Core Frameworks
- **VeritasFi (2025)** â€” Hybrid retrieval + reranking for financial QA.  
  *Informed PennyBotâ€™s hybrid retriever design (CAKC, reranker practices).*

- **Multiâ€‘HyDE (2025)** â€” Hypothetical document embeddings.  
  *Inspired multiâ€‘hop reasoning, query diversification, and recall curve tracking.*

- **FinSage (2025)** â€” Multiâ€‘modal retrieval, hallucination reduction.  
  *Guided hallucination taxonomy, DPO reranker, and complianceâ€‘critical retrieval.*

- **Financial Report Chunking for Effective RAG (2024)** â€” Elementâ€‘based chunking.  
  *Anchored PennyBotâ€™s elementâ€‘aware chunking and metadata logging.*

- **FinQANet (2022)** â€” Programâ€‘ofâ€‘thought reasoning.  
  *Influenced stepâ€‘byâ€‘step reasoning and interpretable outputs.*

---

## Baselines
- **LightRAG (2022)** â€” Dense retrieval baseline.  
- **GraphRAG (2022)** â€” Graphâ€‘structured retrieval baseline.  
- **BM25 (2009)** â€” Sparse retrieval baseline.  
- **FAISS (2017)** â€” Dense retrieval baseline.  
- **BM25 + FAISS (2019)** â€” Hybrid sparseâ€‘dense baseline.  

*These baselines contextualize PennyBotâ€™s resurrection: moving beyond dense/sparse hybrids into agentic orchestration.*

---

## Statistical Methods
- **Efron & Tibshirani (1993)** â€” Bootstrap confidence intervals.  
  *Used for reproducible paired comparisons.*  

- **Wilcoxon (1945)** â€” Signedâ€‘rank test.  
  *Applied for nonparametric paired EM/F1 comparisons.*
  
