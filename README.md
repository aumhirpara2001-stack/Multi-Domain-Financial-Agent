# PennyBot: LLM Agentic RAG

**A production-ready financial question-answering chatbot using Retrieval-Augmented Generation (RAG)**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Mathematical Foundations](#mathematical-foundations)
- [Cost Analysis](#cost-analysis)
- [Evaluation Metrics](#evaluation-metrics)
- [Docker Deployment](#docker-deployment)
- [Usage Examples](#usage-examples)
- [References](#references)

---

## Overview

PennyBot is an **LLM-powered Agentic RAG system** designed for financial question-answering. It combines:

- **Dense Vector Retrieval** using Pinecone and FAISS
- **Together AI** for cost-efficient embeddings and LLM inference
- **LangChain LCEL** for orchestration and conversational context
- **Hallucination Detection** with taxonomy logging
- **TTFT & Latency Tracking** for performance monitoring

### Key Capabilities

- Answers questions on corporate finance, accounting, quantitative finance, and portfolio theory
- Retrieves relevant context from a vector database of 10,000+ financial Q&A pairs
- Cites sources with metadata for transparency
- Maintains conversational context across multi-turn interactions
- Tracks performance metrics (EM, F1, TTFT, hallucination rate)

---

## Features

âœ… **Agentic RAG Pipeline** - Contextualizes queries, retrieves relevant documents, generates grounded answers
âœ… **Conversational Memory** - Maintains chat history and resolves vague references
âœ… **Citation Tracking** - Returns source metadata with every response
âœ… **Dockerized Deployment** - Reproducible builds with GPU support
âœ… **Evaluation Harness** - Automated benchmarking with EM, F1, and hallucination detection
âœ… **Cost Optimized** - ~$0.05/query using Together AI and Pinecone serverless

---

## Project Structure

```
PennyBot_LLM_Agentic_RAG/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_agent_library.py      # Core RAG orchestration (LCEL)
â”‚   â”œâ”€â”€ chat_cli.py               # Interactive CLI interface
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ etl.py                # Data cleaning & preprocessing
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ ingest_and_filter.py      # Load and clean CSV data
â”‚   â”œâ”€â”€ build_index.py            # Populate Pinecone index
â”‚   â”œâ”€â”€ generate_corpus.py        # Synthetic data generation
â”‚   â””â”€â”€ evaluate.py               # Evaluation harness
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ raw/                      # Raw datasets
â”‚   â”‚   â”œâ”€â”€ all_questions_tagged.csv
â”‚   â”‚   â””â”€â”€ financebench_open_source.jsonl
â”‚   â””â”€â”€ processed/                # Cleaned data outputs
â”œâ”€â”€ config/                       # Configuration
â”‚   â””â”€â”€ .env.example              # Environment template
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ tests/                        # Unit tests (future)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml            # Multi-service orchestration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Quick Start

### 1. Prerequisites

- Python 3.10+
- API keys from:
  - [Together AI](https://api.together.xyz/) (for embeddings & LLM)
  - [Pinecone](https://www.pinecone.io/) (for vector database)

### 2. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/PennyBot_LLM_Agentic_RAG.git
cd PennyBot_LLM_Agentic_RAG

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp config/.env.example .env

# Edit .env with your API keys
TOGETHER_API_KEY=your_together_key_here
PINECONE_API_KEY=your_pinecone_key_here
```

### 4. Build Vector Index

```bash
# Process raw data
python scripts/ingest_and_filter.py

# Build Pinecone index (one-time setup)
python scripts/build_index.py
```

### 5. Run the Chatbot

```bash
# Launch interactive CLI
python src/chat_cli.py
```

### 6. Run Evaluation

```bash
# Evaluate on benchmark dataset
python scripts/evaluate.py --dataset data/raw/all_questions_tagged.csv --limit 100

# Full evaluation (no limit)
python scripts/evaluate.py
```

---

## Mathematical Foundations

### Document Chunking

Let D = {dâ‚, dâ‚‚, ..., dâ‚™} be a dataset of documents. Each document dáµ¢ is segmented into smaller chunks cáµ¢â±¼:

**C = {câ‚â‚, câ‚â‚‚, ..., câ‚™â‚˜}**

### Embedding Function

Each chunk c âˆˆ C is mapped to a high-dimensional vector space:

**vá´„ = f(c) âˆˆ â„áµˆ**

Where f is the embedding model (BAAI/bge-base-en-v1.5, d=768).

### Similarity Search

Cosine similarity between query q and chunk vá´„:

**sim(q, vá´„) = (q Â· vá´„) / (â€–qâ€– Â· â€–vá´„â€–)**

### Retrieval

Retrieve top-k most similar chunks:

**R(q) = arg topâ‚– sim(q, vá´„) for c âˆˆ C**

### Augmented Generation

Concatenate retrieved context with query and pass to LLM:

**Answer(q) = LLM(q âŠ• R(q))**

Where âŠ• denotes concatenation.

### Weighted Context Fusion

**P(q) = q âŠ• Î£áµ¢â‚Œâ‚áµ Î±áµ¢ Â· cáµ¢**

Where Î±áµ¢ are weights based on similarity scores.

### Time to First Token (TTFT)

**TTFT = t_first - t_request**

**Total Latency = t_last - t_request**

### Hallucination Taxonomy

```
H(x) = {
  0: grounded in retrieved context
  1: unsupported numeric claim
  2: unsupported textual claim
}
```

---

## Cost Analysis

### Token Cost Function

**Cost_tokens = Î» Â· InputTokens + Î¼ Â· OutputTokens**

Where Î» and Î¼ are provider-specific rates.

### Retrieval Cost Function

**Cost_retrieval = Î± Â· k + Î² Â· Latency**

Where k is the number of retrieved chunks.

### Total Pipeline Cost

**Cost_total = Cost_tokens + Cost_retrieval + Energy_CUDA**

### Cost Estimates (Approximate)

| Component | Cost per 1K Queries |
|-----------|---------------------|
| Together AI Embeddings | ~$0.02 |
| Together AI LLM (Llama-3-70B) | ~$0.30 |
| Pinecone Storage | ~$0.25/GB/month |
| **End-to-End Pipeline** | **~$0.05/query** |

*Target: 84.5% accuracy, 100% coverage*

---

## Evaluation Metrics

The evaluation harness (`scripts/evaluate.py`) computes:

1. **Exact Match (EM)** - Binary check if normalized prediction equals ground truth
2. **Token F1** - Harmonic mean of precision/recall over token overlap
3. **TTFT** - Time to first token (ms)
4. **Total Latency** - End-to-end response time (ms)
5. **Hallucination Rate** - % grounded vs. unsupported claims
6. **Token Usage** - Input/output token counts

Results are saved to `results_tagged.csv` with per-question details.

### Sample Output

```
============================================================
EVALUATION SUMMARY
============================================================
Exact Match:            72.45%
Token F1:               84.50%
Avg Total Latency:      1,234.5 ms
Avg TTFT:               123.4 ms
Grounded Responses:     87.3%
Unsupported Numeric:    8.2%
Unsupported Claims:     4.5%
Avg Docs Retrieved:     3.0
============================================================
```

---

## Docker Deployment

### Build Docker Image

```bash
docker build -t pennybot .
```

### Run with Docker

```bash
# Basic run (evaluation mode)
docker run -it --env-file .env pennybot

# Interactive chat mode
docker run -it --env-file .env pennybot python src/chat_cli.py

# Mount volumes for data persistence
docker run -it -v $(pwd)/data:/app/data --env-file .env pennybot

# GPU acceleration (requires nvidia-docker2)
docker run --gpus all -it --env-file .env pennybot
```

### Docker Compose (Multi-Service)

```bash
# Coming soon: Redis caching + Prometheus monitoring
docker-compose up
```

---

## Usage Examples

### Interactive Chat

```
You: What is Return on Equity (ROE)?

AI: Thinking...

AI: Return on Equity (ROE) is calculated as:

ROE = Net Income Ã· Shareholder's Equity

It measures a company's profitability relative to equity invested.
Higher ROE indicates more efficient use of shareholder capital.

--- Citations ---
Source: synthetic_finance_council, ID: synthetic_00001
-----------------

You: How is it different from ROA?

AI: Thinking...

AI: ROE (Return on Equity) measures profitability relative to shareholder
equity, while ROA (Return on Assets) measures profitability relative to
total assets. Key differences:

â€¢ ROE = Net Income / Equity
â€¢ ROA = Net Income / Total Assets
â€¢ ROE reflects leverage; ROA does not

A company with high debt will have higher ROE than ROA.

--- Citations ---
Source: synthetic_finance_council, ID: synthetic_00023
-----------------
```

### Programmatic Usage

```python
from src.rag_agent_library import (
    get_pinecone_vectorstore,
    create_rag_pipeline
)
from langchain_together import ChatTogether, TogetherEmbeddings

# Initialize
llm = ChatTogether(model="meta-llama/Llama-3-70b-chat-hf", temperature=0.1)
embeddings = TogetherEmbeddings(model="BAAI/bge-base-en-v1.5")

# Get vector store
vectorstore = get_pinecone_vectorstore(embeddings)
retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

# Create RAG agent
rag_agent = create_rag_pipeline(retriever, llm)

# Query
response = rag_agent.invoke({
    "question": "How is WACC calculated?",
    "chat_history": []
})

print(response['answer'])
print(f"Retrieved {len(response['retrieved_docs'])} documents")
```

---

## References

### Core Frameworks

- **VeritasFi (2025)** - Hybrid retrieval + reranking for financial QA
- **Multi-HyDE (2025)** - Hypothetical document embeddings for multi-hop reasoning
- **FinSage (2025)** - Multi-modal retrieval with hallucination reduction
- **Financial Report Chunking (2024)** - Element-based chunking for financial docs
- **FinQANet (2022)** - Program-of-thought reasoning for financial questions

### Baselines

- **LightRAG** - Dense retrieval baseline
- **GraphRAG** - Graph-structured retrieval
- **BM25** - Sparse keyword retrieval
- **FAISS** - Facebook AI Similarity Search
- **Hybrid (BM25 + FAISS)** - Combined sparse-dense retrieval

### Statistical Methods

- **Efron & Tibshirani (1993)** - Bootstrap confidence intervals
- **Wilcoxon (1945)** - Signed-rank test for paired comparisons

---

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black src/ scripts/

# Lint
flake8 src/ scripts/
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

Built with:
- [LangChain](https://python.langchain.com/) - RAG orchestration
- [Pinecone](https://www.pinecone.io/) - Vector database
- [Together AI](https://www.together.ai/) - LLM inference & embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Similarity search

---

## Contact

For questions or feedback, please open an issue on GitHub.

**PennyBot** - Making financial knowledge accessible through AI ğŸ¤–ğŸ“Š
