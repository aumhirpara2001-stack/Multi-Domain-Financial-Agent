
---


PennyBot_LLM_Agentic_RAG

**PennyBot reborn as an LLM‚ÄëAgentic RAG Chatbot**  
Dockerized, CUDA‚Äëaccelerated, TTFT tracked, hallucination taxonomy logged, and orchestrated end‚Äëto‚Äëend with sustainable low‚Äëtoken, low‚Äëenergy retrieval.

---

üìò Part I. Mathematical Foundations (Textbook Mode)


---

## üìê Mathematical Foundations

### 1. Document Chunking

Let \( D = \{d‚ÇÅ, d‚ÇÇ, \dots, d‚Çô\} \) be a dataset of documents. Each document \( d·µ¢ \) is segmented into smaller textual chunks \( c·µ¢‚±º \), forming a new collection:

<p align="center"><strong>C = {c‚ÇÅ‚ÇÅ, c‚ÇÅ‚ÇÇ, ..., c‚Çô‚Çò}</strong></p>

---

### 2. Embedding Function

Each chunk \( c \in C \) is mapped into a high-dimensional vector space via an embedding function \( f \):

<p align="center"><strong>v<sub>c</sub> = f(c) ‚àà ‚Ñù·µà</strong></p>

---

### 3. Vector Store Construction

All chunk embeddings are stored in a FAISS index:

<p align="center"><strong>V = {v<sub>c‚ÇÅ</sub>, v<sub>c‚ÇÇ</sub>, ..., v<sub>c‚Çñ</sub>}</strong></p>

Similarity between a query vector \( q \) and a chunk vector \( v_c \) is computed using cosine similarity:

<p align="center"><strong>sim(q, v<sub>c</sub>) = (q ¬∑ v<sub>c</sub>) / (‚Äñq‚Äñ ¬∑ ‚Äñv<sub>c</sub>‚Äñ)</strong></p>

---

### 4. Retrieval

Given a user query \( q \), we first embed it:

<p align="center"><strong>q = f(q)</strong></p>

We then retrieve the top‚Äëk most similar chunks:

<p align="center"><strong>R(q) = arg<sub>top‚Äëk</sub><sub>c ‚àà C</sub> sim(q, v<sub>c</sub>)</strong></p>

---

### 5. Augmented Generation

The retrieved chunks \( R(q) \) are concatenated with the query and passed to the language model:

<p align="center"><strong>Answer(q) = LLM(q ‚äï R(q))</strong></p>

Here, ‚äï denotes the concatenation of the query and its retrieved context.

---

## Part II. Codebook Translation (Developer Manual)

### 1. Environment Setup
```bash
pip install langchain==0.3.7 langchain-community==0.3.7 \
            langchain-openai==0.3.7 langchain-together==0.3.7 \
            faiss-cpu python-dotenv pandas datasets scikit-learn tqdm PyYAML streamlit
```

---

### 2. `.env` File
```dotenv
OPENAI_API_KEY=your_openai_key
TOGETHER_API_KEY=your_together_key
EMBEDDING_PROVIDER=openai
```

---
---


6. Evaluation Metrics
- **Exact Match (EM)**: binary check if normalized prediction = gold.  
- **Token F1**: harmonic mean of precision/recall over token overlap.  
- **TTFT**: time to first token.  
- **Total Latency**: end‚Äëto‚Äëend wall‚Äëclock time.  
- **Hallucination Taxonomy**: {grounded, unsupported_numeric, unsupported_claim}.  


üìê Prompt Engineering Math


### Weighted Context Fusion


\[
P(q) = q \oplus \sum_{i=1}^k \alpha_i \cdot c_i
\]


- \(q\) = query  
- \(c_i\) = retrieved chunk  
- \(\alpha_i\) = weight coefficient (similarity, token budget, energy cost)

### Token + Energy Cost Function


\[
\text{Cost}(R) = \lambda \cdot \text{Tokens}(R) + \mu \cdot \text{Energy}(R)
\]



### TTFT Metric


\[
\text{TTFT} = t_{\text{first}} - t_{\text{request}}
\]




\[
\text{Latency} = t_{\text{last}} - t_{\text{request}}
\]



### Hallucination Taxonomy


\[
H(x) =
\begin{cases}
0 & \text{grounded in retrieved context} \\
1 & \text{unsupported numeric claim} \\
2 & \text{unsupported textual claim}
\end{cases}
\]



### Constraint‚ÄëDriven Prompt


\[
\text{Prompt}(q) = \text{LLM}(q \oplus R(q) \mid \text{Constraints})
\]


---

## üîë API Keys

To run PennyBot_LLM_Agentic_RAG you‚Äôll need free API keys:

- [Together AI](https://api.together.xyz/) ‚Üí for cost‚Äëefficient embeddings and hosted inference
- [Pinecone](https://www.pinecone.io/) ‚Üí for scalable vector database
- (Optional) Hugging Face Hub ‚Üí for dataset pulls and model hosting

Add them to your `.env` file:

TOGETHER_API_KEY=your_together_key  
PINECONE_API_KEY=your_pinecone_key

---

Cost-Benefit Analysis

Yes ‚Äî if this README is going to be a **saga**, it needs both the *practical links* (where to grab free API keys) and the *numerical testimony* (your end‚Äëto‚Äëend cost slicing). Right now it reads like a textbook, but you want it to feel like a fellowship epic: math, code, lore, and economics all braided together.


---


## üí∏ End‚Äëto‚ÄëEnd Cost Optimization



### 1. Token Cost Function


\[
\text{Cost}_{\text{tokens}} = \lambda \cdot \text{InputTokens} + \mu \cdot \text{OutputTokens}
\]



### 2. Retrieval Cost Function


\[
\text{Cost}_{\text{retrieval}} = \alpha \cdot k + \beta \cdot \text{Latency}
\]



### 3. Total Pipeline Cost


\[
\text{Cost}_{\text{total}} = \text{Cost}_{\text{tokens}} + \text{Cost}_{\text{retrieval}} + \text{Energy}_{\text{CUDA}}
\]


---

## üìä Approximations - Subject to Change

- **OpenAI embeddings**: ~$0.10 per 1K queries (high‚Äëfidelity, but pricier).  
- **Together embeddings**: ~$0.02 per 1K queries (optimized, fellowship‚Äëgrade).  
- **Pinecone storage**: ~$0.25 per GB/month (scales with corpus size).  
- **CUDA acceleration**: negligible marginal cost once GPU is provisioned.  
- **End‚Äëto‚Äëend pipeline**: you benchmarked ~84.5% accuracy with **100% coverage** at **< $0.05/query**.

---


## üìë Part II. Codebook Translation (Developer Manual)

### Environment Setup
```bash
pip install langchain==0.3.7 langchain-community==0.3.7 \
            langchain-openai==0.3.7 langchain-together==0.3.7 \
            faiss-cpu python-dotenv pandas datasets scikit-learn tqdm PyYAML
````

### .env File

```
.env

TOGETHER_API_KEY=your_together_key
EMBEDDING_PROVIDER=openai
```

### Retrieval + Generation

```
python
retriever = get_retriever(index_path)
docs = retriever.retrieve(query, top_k=5)
chunks = [d.page_content for d in docs]
gen_resp = call_rag_generator(query, chunks)
```

### Evaluation Harness
- Logs EM, F1, hallucination type, complexity flag.  
- Tracks TTFT, total latency, input/output tokens.  
- Appends results to `results_tagged.csv`.  
- Prints summary averages for fellowship‚Äëgrade reproducibility.

---

## ‚úÖ Summary
PennyBot‚Äôs resurrection is not just a chatbot. It is:
- A **CUDA‚Äëpowered, Docker‚Äëhardened RAG pipeline**.  
- A **mathematical textbook** (Part I) and **developer codebook** (Part II).  
- A **fellowship artifact**: every eval request stamped with time, tokens, hallucination taxonomy, and reproducibility.



---

# üìö References

## Core Frameworks
- **VeritasFi (2025)** ‚Äî Hybrid retrieval + reranking for financial QA.  
  *Informed PennyBot‚Äôs hybrid retriever design (CAKC, reranker practices).*

- **Multi‚ÄëHyDE (2025)** ‚Äî Hypothetical document embeddings.  
  *Inspired multi‚Äëhop reasoning, query diversification, and recall curve tracking.*

- **FinSage (2025)** ‚Äî Multi‚Äëmodal retrieval, hallucination reduction.  
  *Guided hallucination taxonomy, DPO reranker, and compliance‚Äëcritical retrieval.*

- **Financial Report Chunking for Effective RAG (2024)** ‚Äî Element‚Äëbased chunking.  
  *Anchored PennyBot‚Äôs element‚Äëaware chunking and metadata logging.*

- **FinQANet (2022)** ‚Äî Program‚Äëof‚Äëthought reasoning.  
  *Influenced step‚Äëby‚Äëstep reasoning and interpretable outputs.*

---

## Baselines
- **LightRAG (2022)** ‚Äî Dense retrieval baseline.  
- **GraphRAG (2022)** ‚Äî Graph‚Äëstructured retrieval baseline.  
- **BM25 (2009)** ‚Äî Sparse retrieval baseline.  
- **FAISS (2017)** ‚Äî Dense retrieval baseline.  
- **BM25 + FAISS (2019)** ‚Äî Hybrid sparse‚Äëdense baseline.  

*These baselines contextualize PennyBot‚Äôs resurrection: moving beyond dense/sparse hybrids into agentic orchestration.*

---

## Statistical Methods
- **Efron & Tibshirani (1993)** ‚Äî Bootstrap confidence intervals.  
  *Used for reproducible paired comparisons.*  

- **Wilcoxon (1945)** ‚Äî Signed‚Äërank test.  
  *Applied for nonparametric paired EM/F1 comparisons.*
  
