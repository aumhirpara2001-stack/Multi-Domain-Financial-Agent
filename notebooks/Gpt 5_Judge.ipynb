{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8a8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI client ready.\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, random, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import faiss\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"âœ… OpenAI client ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cea30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded: (7000, 5)\n",
      "                                            question  \\\n",
      "0  What area did NVIDIA initially focus on before...   \n",
      "1  What are some of the recent applications of GP...   \n",
      "2  What significant invention did NVIDIA create i...   \n",
      "\n",
      "                                              answer  \\\n",
      "0           NVIDIA initially focused on PC graphics.   \n",
      "1  Recent applications of GPU-powered deep learni...   \n",
      "2                   NVIDIA invented the GPU in 1999.   \n",
      "\n",
      "                                             context ticker    filing  \n",
      "0  Since our original focus on PC graphics, we ha...   NVDA  2023_10K  \n",
      "1  Some of the most recent applications of GPU-po...   NVDA  2023_10K  \n",
      "2  Our invention of the GPU in 1999 defined moder...   NVDA  2023_10K  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-QA-10k.csv\")\n",
    "df = df.fillna(\"\")\n",
    "print(f\"âœ… Dataset loaded: {df.shape}\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3366b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [07:45<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "context_embeddings = embed_model.encode(df[\"context\"].tolist(), show_progress_bar=True)\n",
    "index = faiss.IndexFlatL2(context_embeddings.shape[1])\n",
    "index.add(context_embeddings)\n",
    "\n",
    "def retrieve_contexts(question, top_k=1):\n",
    "    q_vec = embed_model.encode([question])\n",
    "    D, I = index.search(q_vec, top_k)\n",
    "    return [df.iloc[i][\"context\"] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What significant invention did NVIDIA create in 1999?\",\n",
      "  \"answer\": \"NVIDIA created the GPU (Graphics Processing Unit) in 1999.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#  Answer Generation â€” GPT-4o-mini\n",
    "def generate_answer_llm(question, context=None, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Use GPT-4o-mini to generate concise financial answers.\"\"\"\n",
    "    if context:\n",
    "        prompt = f\"\"\"\n",
    "You are a financial analyst AI assistant.\n",
    "Using the following financial filing excerpt, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If context lacks sufficient data, respond with \"Not enough information in the provided text.\"\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"You are a financial analyst. Answer clearly and factually:\\n\\n{question}\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Error (generator):\", e)\n",
    "        return None\n",
    "\n",
    "#  Quick Test â€” Generate Financial Answer (GPT-4o-mini)\n",
    "\n",
    "sample_q = \"What significant invention did NVIDIA create in 1999?\"\n",
    "sample_ctx = retrieve_contexts(sample_q, top_k=1)[0]\n",
    "\n",
    "out = generate_answer_llm(sample_q, context=sample_ctx, model=\"gpt-4o-mini\")\n",
    "print(json.dumps({\"question\": sample_q, \"answer\": out}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 50 questions for batch generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:53<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saved batch results to /Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT4oMini-QA_batch_0.csv\n",
      "âœ… Generated 50 predictions successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Batch Generation â€” GPT-4o-mini Answers\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "INPUT_PATH  = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-QA-10k.csv\"\n",
    "OUTPUT_PATH = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT4oMini-QA.csv\"\n",
    "\n",
    "BATCH_SIZE  = 50     # adjust to 100â€“200 if you want longer runs\n",
    "START_INDEX = 0      # change to resume later\n",
    "generated_answers = []\n",
    "\n",
    "# Load input file\n",
    "df_input = pd.read_csv(INPUT_PATH).fillna(\"\")\n",
    "df_slice = df_input.iloc[START_INDEX:START_INDEX + BATCH_SIZE].copy()\n",
    "\n",
    "print(f\"âœ… Loaded {len(df_slice)} questions for batch generation.\")\n",
    "\n",
    "for i, row in tqdm(df_slice.iterrows(), total=len(df_slice)):\n",
    "    q = row[\"question\"]\n",
    "    try:\n",
    "        ctx = retrieve_contexts(q, top_k=1)[0]\n",
    "        ans = generate_answer_llm(q, context=ctx, model=\"gpt-4o-mini\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error at row {i}: {e}\")\n",
    "        ans = None\n",
    "    generated_answers.append(ans)\n",
    "    time.sleep(2)  # small pause for rate-limit safety\n",
    "\n",
    "# Attach predictions\n",
    "df_slice[\"gpt4o_mini_pred\"] = generated_answers\n",
    "\n",
    "# Save checkpoint batch\n",
    "output_file = OUTPUT_PATH.replace(\".csv\", f\"_batch_{START_INDEX}.csv\")\n",
    "df_slice.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved batch results to {output_file}\")\n",
    "print(f\"âœ… Generated {len(df_slice)} predictions successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GPT-5 Semantic Judge â€” Structured & Clean Output\n",
    "import re, json, time\n",
    "\n",
    "def llm_as_judge(question, true_answer, pred_answer, model=\"gpt-5\"):\n",
    "    \"\"\"\n",
    "    GPT-5 evaluates semantic quality between the gold and predicted answers.\n",
    "    Returns (score, explanation). Enforces strict JSON output.\n",
    "    \"\"\"\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert financial QA evaluator. \"\n",
    "            \"You must respond ONLY with valid JSON following this schema:\\n\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"score\": float between 0 and 1,\\n'\n",
    "            '  \"explanation\": \"short reason (<40 words)\"\\n'\n",
    "            \"}\\n\\n\"\n",
    "            \"No extra text, headers, or commentary.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"True Answer: {true_answer}\\n\\n\"\n",
    "            f\"Predicted Answer: {pred_answer}\\n\\n\"\n",
    "            \"Evaluate correctness, completeness, and relevance. \"\n",
    "            \"Assign one combined score from 0.0â€“1.0.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[system_msg, user_msg],\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "        # Try direct JSON load\n",
    "        try:\n",
    "            result = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            # Extract JSON-like content if extra text leaked\n",
    "            match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
    "            result = json.loads(match.group(0)) if match else {\"score\": 0.0, \"explanation\": raw[:80]}\n",
    "\n",
    "        score = float(result.get(\"score\", 0.0))\n",
    "        reason = result.get(\"explanation\", \"\").strip()\n",
    "        return score, reason\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Judge error:\", e)\n",
    "        return 0.0, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2931eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 10/50 [01:01<03:44,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:48<02:10,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [02:30<01:22,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [03:16<00:43,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:17<00:00,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint 50\n",
      "âœ… Completed evaluation â†’ /Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT5-Judge-Results.csv\n",
      "Average Judge Score: 0.220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "save_path = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT5-Judge-Results.csv\"\n",
    "\n",
    "scores, reasons, answers = [], [], []\n",
    "gen_model = \"gpt-4o-mini\"\n",
    "judge_model = \"gpt-5\"\n",
    "\n",
    "for i, row in tqdm(df.head(50).iterrows(), total=50):\n",
    "    q, true_a = row[\"question\"], row[\"answer\"]\n",
    "    ctx = retrieve_contexts(q, top_k=1)[0]\n",
    "\n",
    "    # --- Generate with GPT-4o-mini ---\n",
    "    try:\n",
    "        pred = generate_answer_llm(q, context=ctx, model=gen_model)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Generation error: {e}\")\n",
    "        pred = \"\"\n",
    "\n",
    "    # --- Judge with GPT-5 ---\n",
    "    for attempt in range(3):\n",
    "        s, r = llm_as_judge(q, true_a, pred, model=judge_model)\n",
    "        if s is not None:\n",
    "            break\n",
    "        print(\"â³ Retry due to transient error...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    answers.append(pred)\n",
    "    scores.append(s)\n",
    "    reasons.append(r)\n",
    "\n",
    "    # --- Save every 10 samples ---\n",
    "    if (i + 1) % 10 == 0:\n",
    "        pd.DataFrame({\n",
    "            \"question\": df.head(i + 1)[\"question\"].values,\n",
    "            \"true_answer\": df.head(i + 1)[\"answer\"].values,\n",
    "            \"pred_answer\": answers,\n",
    "            \"score\": scores,\n",
    "            \"reason\": reasons,\n",
    "        }).to_csv(save_path, index=False)\n",
    "        print(f\"ðŸ’¾ Saved checkpoint {i+1}\")\n",
    "\n",
    "# --- Final save ---\n",
    "pd.DataFrame({\n",
    "    \"question\": df.head(len(scores))[\"question\"].values,\n",
    "    \"true_answer\": df.head(len(scores))[\"answer\"].values,\n",
    "    \"pred_answer\": answers,\n",
    "    \"score\": scores,\n",
    "    \"reason\": reasons,\n",
    "}).to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"âœ… Completed evaluation â†’ {save_path}\")\n",
    "print(f\"Average Judge Score: {np.nanmean(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ddfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
