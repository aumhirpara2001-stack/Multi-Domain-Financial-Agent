{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c93e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI version: 1.55.3\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(\"‚úÖ OpenAI version:\", openai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342bde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0256ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Key loaded and exported: sk-proj-dtvl...dV18A\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get key and ensure it's visible to the environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key  # üëà ensures new SDK can auto-detect it\n",
    "\n",
    "if api_key:\n",
    "    print(\"‚úÖ Key loaded and exported:\", api_key[:12] + \"...\" + api_key[-5:])\n",
    "else:\n",
    "    raise ValueError(\"‚ùå No API key found ‚Äî check your .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786aef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()  # now auto-detects the key\n",
    "print(\"‚úÖ Client initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe3adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Key loaded: sk-proj-dtvl...dV18A\n",
      "‚úÖ Dataset loaded: (7000, 5)\n",
      "                                            question  \\\n",
      "0  What area did NVIDIA initially focus on before...   \n",
      "1  What are some of the recent applications of GP...   \n",
      "2  What significant invention did NVIDIA create i...   \n",
      "\n",
      "                                              answer  \\\n",
      "0           NVIDIA initially focused on PC graphics.   \n",
      "1  Recent applications of GPU-powered deep learni...   \n",
      "2                   NVIDIA invented the GPU in 1999.   \n",
      "\n",
      "                                             context ticker    filing  \n",
      "0  Since our original focus on PC graphics, we ha...   NVDA  2023_10K  \n",
      "1  Some of the most recent applications of GPU-po...   NVDA  2023_10K  \n",
      "2  Our invention of the GPU in 1999 defined moder...   NVDA  2023_10K  \n",
      "‚úÖ Mock model predictions added for Mistral & LLaMA.\n",
      "\n",
      "üîé Evaluating model: mistral_pred with judge: gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [15:57<00:00, 19.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mistral_pred ‚Üí Average Judge Score: 0.018\n",
      "\n",
      "üîé Evaluating model: llama_pred with judge: gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [22:12<00:00, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama_pred ‚Üí Average Judge Score: 0.028\n",
      "\n",
      "üíæ Results saved to:\n",
      "  ‚Ä¢ Financial-Agent-LLM-Judge-Results.csv\n",
      "  ‚Ä¢ Financial-Agent-LLM-Detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1Ô∏è‚É£ Imports & Setup\n",
    "# =========================================================\n",
    "from dotenv import load_dotenv\n",
    "import os, pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import random, json\n",
    "import time\n",
    "time.sleep(25)   # wait 25 seconds between calls\n",
    "\n",
    "\n",
    "# Load OpenAI Key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"‚úÖ Key loaded:\", api_key[:12] + \"...\" + api_key[-5:])\n",
    "else:\n",
    "    raise ValueError(\"‚ùå No API key found. Please add OPENAI_API_KEY to your .env\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# =========================================================\n",
    "# 2Ô∏è‚É£ Load Dataset\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-QA-10k.csv\")\n",
    "df = df.fillna(\"\")\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape}\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Just in case columns differ\n",
    "text_col = \"context\"\n",
    "question_col = \"question\"\n",
    "answer_col = \"answer\"\n",
    "\n",
    "# =========================================================\n",
    "# 3Ô∏è‚É£ Generate Model Predictions (Baseline LLMs)\n",
    "# =========================================================\n",
    "# You can replace these with actual calls to local models (Mistral / LLaMA)\n",
    "# For now, we simulate random model predictions\n",
    "def mock_model_prediction(question):\n",
    "    endings = [\n",
    "        \"increased financial stability.\",\n",
    "        \"improved risk-adjusted returns.\",\n",
    "        \"lowered capital costs.\",\n",
    "        \"helped diversification efforts.\"\n",
    "    ]\n",
    "    return f\"The company's strategy led to {random.choice(endings)}\"\n",
    "\n",
    "df[\"mistral_pred\"] = df[question_col].apply(lambda q: mock_model_prediction(q))\n",
    "df[\"llama_pred\"] = df[question_col].apply(lambda q: mock_model_prediction(q))\n",
    "\n",
    "print(\"‚úÖ Mock model predictions added for Mistral & LLaMA.\")\n",
    "\n",
    "# =========================================================\n",
    "# 4Ô∏è‚É£ Define the LLM-as-a-Judge function\n",
    "# =========================================================\n",
    "def llm_as_judge(question, true_answer, pred_answer, judge_model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Use ChatGPT (GPT-5/4o-mini) to semantically evaluate model prediction quality.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a financial QA evaluation assistant.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth Answer: {true_answer}\n",
    "Predicted Answer: {pred_answer}\n",
    "\n",
    "Evaluate the predicted answer from 0.0 to 1.0 based on:\n",
    "1. **Correctness** ‚Äì factual alignment with the ground truth.\n",
    "2. **Completeness** ‚Äì inclusion of key details.\n",
    "3. **Relevance** ‚Äì focus on answering the question.\n",
    "\n",
    "Return only valid JSON:\n",
    "{{\"score\": 0.xx, \"justification\": \"short reasoning\"}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=judge_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        result = json.loads(content)\n",
    "        return result.get(\"score\", 0.0), result.get(\"justification\", \"\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error:\", e)\n",
    "        return None, str(e)\n",
    "\n",
    "# =========================================================\n",
    "# 5Ô∏è‚É£ Evaluate Mistral & LLaMA Predictions\n",
    "# =========================================================\n",
    "results = []\n",
    "judge_model = \"gpt-4o-mini\"  # or \"gpt-5\" if available\n",
    "\n",
    "for model_name in [\"mistral_pred\", \"llama_pred\"]:\n",
    "    print(f\"\\nüîé Evaluating model: {model_name} with judge: {judge_model}\")\n",
    "    scores, reasons = [], []\n",
    "\n",
    "    for _, row in tqdm(df.head(50).iterrows(), total=50):\n",
    "        s, r = llm_as_judge(\n",
    "            question=row[question_col],\n",
    "            true_answer=row[answer_col],\n",
    "            pred_answer=row[model_name],\n",
    "            judge_model=judge_model\n",
    "        )\n",
    "        scores.append(s)\n",
    "        reasons.append(r)\n",
    "\n",
    "   # only attach to evaluated subset\n",
    "    df_subset = df.head(len(scores)).copy()\n",
    "    df_subset[f\"{model_name}_score\"] = scores\n",
    "    df_subset[f\"{model_name}_reason\"] = reasons\n",
    "\n",
    "    avg_score = np.mean([s for s in scores if s is not None])\n",
    "    print(f\"‚úÖ {model_name} ‚Üí Average Judge Score: {avg_score:.3f}\")\n",
    "\n",
    "    results.append({\"model\": model_name, \"avg_score\": avg_score, \"judge\": judge_model})\n",
    "\n",
    "# =========================================================\n",
    "# 6Ô∏è‚É£ Save Results\n",
    "# =========================================================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-LLM-Judge-Results.csv\", index=False)\n",
    "df.to_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-LLM-Detailed.csv\", index=False)\n",
    "print(\"\\nüíæ Results saved to:\")\n",
    "print(\"  ‚Ä¢ Financial-Agent-LLM-Judge-Results.csv\")\n",
    "print(\"  ‚Ä¢ Financial-Agent-LLM-Detailed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ddc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
