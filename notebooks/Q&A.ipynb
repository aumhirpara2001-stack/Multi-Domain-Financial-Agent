{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "#Imports & Environment Setup\n",
    "import os, time, json, random, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import faiss\n",
    "\n",
    "# Load key from .env (make sure .env has: OPENAI_API_KEY=sk-...)\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert api_key, \"‚ùå No API key found. Add OPENAI_API_KEY to your .env file.\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"‚úÖ OpenAI client initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be31c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset: (7000, 5)\n",
      "                                            question  \\\n",
      "0  What area did NVIDIA initially focus on before...   \n",
      "1  What are some of the recent applications of GP...   \n",
      "2  What significant invention did NVIDIA create i...   \n",
      "\n",
      "                                              answer  \\\n",
      "0           NVIDIA initially focused on PC graphics.   \n",
      "1  Recent applications of GPU-powered deep learni...   \n",
      "2                   NVIDIA invented the GPU in 1999.   \n",
      "\n",
      "                                             context ticker    filing  \n",
      "0  Since our original focus on PC graphics, we ha...   NVDA  2023_10K  \n",
      "1  Some of the most recent applications of GPU-po...   NVDA  2023_10K  \n",
      "2  Our invention of the GPU in 1999 defined moder...   NVDA  2023_10K  \n"
     ]
    }
   ],
   "source": [
    "#Load \n",
    "CSV_PATH = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-QA-10k.csv\"\n",
    "df = pd.read_csv(CSV_PATH).fillna(\"\")\n",
    "print(\"‚úÖ Dataset:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# dataset already uses these:\n",
    "QUESTION_COL = \"question\"\n",
    "ANSWER_COL   = \"answer\"\n",
    "CONTEXT_COL  = \"context\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0124fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Building FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [06:18<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built with 7000 vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieval-Augmented Financial QA from Transformer & reader model\n",
    "print(\"üîç Building FAISS index...\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "context_embeddings = embed_model.encode(df[\"context\"].tolist(), show_progress_bar=True)\n",
    "dim = context_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(context_embeddings)\n",
    "print(f\"‚úÖ FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# Load QA Reader\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "reader_model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "reader = pipeline(\"question-answering\", model=reader_model, tokenizer=tokenizer)\n",
    "\n",
    "def retrieve_contexts(question, top_k=3):\n",
    "    q_vec = embed_model.encode([question])\n",
    "    D, I = index.search(q_vec, top_k)\n",
    "    return [df.iloc[i][\"context\"] for i in I[0]]\n",
    "\n",
    "def answer_question(question):\n",
    "    contexts = retrieve_contexts(question)\n",
    "    answers = []\n",
    "    for ctx in contexts:\n",
    "        ans = reader(question=question, context=ctx)\n",
    "        answers.append(ans)\n",
    "    best = max(answers, key=lambda x: x[\"score\"])\n",
    "    return best[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e93b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Experiment 1 Complete ‚Üí EM=0.000, F1=0.277\n"
     ]
    }
   ],
   "source": [
    "subset = df.head(10).copy()\n",
    "subset[\"retrieved_answer\"] = subset[\"question\"].apply(answer_question)\n",
    "\n",
    "def exact_match(pred, truth):\n",
    "    return int(pred.strip().lower() == truth.strip().lower())\n",
    "\n",
    "def f1_score(pred, truth):\n",
    "    pred_tokens, truth_tokens = pred.lower().split(), truth.lower().split()\n",
    "    common = len(set(pred_tokens) & set(truth_tokens))\n",
    "    if common == 0: return 0\n",
    "    precision, recall = common / len(pred_tokens), common / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "subset[\"EM\"] = subset.apply(lambda r: exact_match(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "subset[\"F1\"] = subset.apply(lambda r: f1_score(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "\n",
    "print(f\"‚úÖ Experiment 1 Complete ‚Üí EM={subset['EM'].mean():.3f}, F1={subset['F1'].mean():.3f}\")\n",
    "subset.to_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Exp1_RetrievalQA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d342d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Experiment 1 Complete ‚Üí EM=0.040, F1=0.248\n"
     ]
    }
   ],
   "source": [
    "subset = df.head(50).copy()\n",
    "subset[\"retrieved_answer\"] = subset[\"question\"].apply(answer_question)\n",
    "\n",
    "def exact_match(pred, truth):\n",
    "    return int(pred.strip().lower() == truth.strip().lower())\n",
    "\n",
    "def f1_score(pred, truth):\n",
    "    pred_tokens, truth_tokens = pred.lower().split(), truth.lower().split()\n",
    "    common = len(set(pred_tokens) & set(truth_tokens))\n",
    "    if common == 0: return 0\n",
    "    precision, recall = common / len(pred_tokens), common / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "subset[\"EM\"] = subset.apply(lambda r: exact_match(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "subset[\"F1\"] = subset.apply(lambda r: f1_score(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "\n",
    "print(f\"‚úÖ Experiment 1 Complete ‚Üí EM={subset['EM'].mean():.3f}, F1={subset['F1'].mean():.3f}\")\n",
    "subset.to_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Exp1_RetrievalQA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753bc4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Experiment 1 Complete ‚Üí EM=0.020, F1=0.207\n"
     ]
    }
   ],
   "source": [
    "subset = df.head(100).copy()\n",
    "subset[\"retrieved_answer\"] = subset[\"question\"].apply(answer_question)\n",
    "\n",
    "def exact_match(pred, truth):\n",
    "    return int(pred.strip().lower() == truth.strip().lower())\n",
    "\n",
    "def f1_score(pred, truth):\n",
    "    pred_tokens, truth_tokens = pred.lower().split(), truth.lower().split()\n",
    "    common = len(set(pred_tokens) & set(truth_tokens))\n",
    "    if common == 0: return 0\n",
    "    precision, recall = common / len(pred_tokens), common / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "subset[\"EM\"] = subset.apply(lambda r: exact_match(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "subset[\"F1\"] = subset.apply(lambda r: f1_score(r[\"retrieved_answer\"], r[\"answer\"]), axis=1)\n",
    "\n",
    "print(f\"‚úÖ Experiment 1 Complete ‚Üí EM={subset['EM'].mean():.3f}, F1={subset['F1'].mean():.3f}\")\n",
    "subset.to_csv(\"/Users/pulinkumar/Desktop/ALGOVERSE/Exp1_RetrievalQA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"How do interest rate hikes affect bond prices?\",\n",
      "  \"best_answer\": \"higher market interest rates offered for retail deposits\",\n",
      "  \"best_confidence\": 0.335,\n",
      "  \"all_context_answers\": [\n",
      "    {\n",
      "      \"context_id\": 1,\n",
      "      \"predicted_answer\": \"increase our future borrowing costs\",\n",
      "      \"confidence\": 0.301,\n",
      "      \"context_snippet\": \"In addition, economic conditions and actions by policymaking bodies are contributing to changing interest rates and significant capital market volatility, which, along with any increases in our borrowing levels, could increase our future borrowing co...\"\n",
      "    },\n",
      "    {\n",
      "      \"context_id\": 2,\n",
      "      \"predicted_answer\": \"higher market interest rates offered for retail deposits\",\n",
      "      \"confidence\": 0.335,\n",
      "      \"context_snippet\": \"The increase in interest rates paid on our deposits were primarily due to the impact of higher market interest rates offered for retail deposits.\"\n",
      "    },\n",
      "    {\n",
      "      \"context_id\": 3,\n",
      "      \"predicted_answer\": \"Interest expense increased, primarily driven by higher interest rates paid on customer deposits\",\n",
      "      \"confidence\": 0.004,\n",
      "      \"context_snippet\": \"Interest expense increased, primarily driven by higher interest rates paid on customer deposits.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Enhanced Quick Test ‚Äî Retrieval QA with per-context scores\n",
    "\n",
    "def ask(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Runs retrieval + reader pipeline on a custom question.\n",
    "    Shows predicted answers and confidence scores for each retrieved context.\n",
    "    \"\"\"\n",
    "    contexts = retrieve_contexts(question, top_k=top_k)\n",
    "    detailed_answers = []\n",
    "\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        ans = reader(question=question, context=ctx)\n",
    "        detailed_answers.append({\n",
    "            \"context_id\": i + 1,\n",
    "            \"predicted_answer\": ans[\"answer\"],\n",
    "            \"confidence\": round(ans[\"score\"], 3),\n",
    "            \"context_snippet\": ctx[:250] + (\"...\" if len(ctx) > 250 else \"\")\n",
    "        })\n",
    "\n",
    "    # pick best answer by confidence\n",
    "    best = max(detailed_answers, key=lambda x: x[\"confidence\"])\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"best_answer\": best[\"predicted_answer\"],\n",
    "        \"best_confidence\": best[\"confidence\"],\n",
    "        \"all_context_answers\": detailed_answers\n",
    "    }\n",
    "\n",
    "# üß† Run a test query\n",
    "sample_q = \"How do interest rate hikes affect bond prices?\"\n",
    "out = ask(sample_q, top_k=3)\n",
    "\n",
    "import json\n",
    "print(json.dumps(out, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb1fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What significant invention did NVIDIA create in 1999?\",\n",
      "  \"answer\": \"NVIDIA created the GPU (Graphics Processing Unit) in 1999.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Direct LLM (gpt-4o) Generated Financial QA\n",
    "from openai import OpenAI\n",
    "import json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def generate_answer_llm(question, context=None, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Uses GPT model to generate financial QA answers.\n",
    "    Optionally includes a retrieved context for grounding.\n",
    "    \"\"\"\n",
    "    if context:\n",
    "        prompt = f\"\"\"\n",
    "You are a financial analyst AI assistant.\n",
    "Using the following financial filing excerpt, answer the question concisely and factually.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the context doesn't contain enough information, state that clearly.\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"You are a financial analyst AI assistant. Answer the question clearly and factually.\\n\\nQuestion: {question}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=250\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# üîé Quick Test\n",
    "sample_q = \"What significant invention did NVIDIA create in 1999?\"\n",
    "sample_ctx = retrieve_contexts(sample_q, top_k=1)[0]\n",
    "out = generate_answer_llm(sample_q, context=sample_ctx)\n",
    "print(json.dumps({\"question\": sample_q, \"answer\": out}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f74c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:05<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved batch results to /Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT4o-QA_batch_0.csv\n",
      "‚úÖ Generated 50 predictions successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch QA Generation For 50 Q & A Using GPT-4o\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "INPUT_PATH  = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-Questions.csv\"\n",
    "OUTPUT_PATH = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-GPT4o-QA.csv\"\n",
    "\n",
    "generated_answers = []\n",
    "BATCH_SIZE = 50    # Change to 100 or 200 for longer runs\n",
    "START_INDEX = 0    # Change if resuming from checkpoint\n",
    "\n",
    "df_slice = df.iloc[START_INDEX:START_INDEX+BATCH_SIZE].copy()\n",
    "\n",
    "for i, row in tqdm(df_slice.iterrows(), total=len(df_slice)):\n",
    "    q = row[\"question\"]\n",
    "    ctx = retrieve_contexts(q, top_k=1)[0]\n",
    "    ans = generate_answer_llm(q, context=ctx, model=\"gpt-4o\")\n",
    "    generated_answers.append(ans)\n",
    "\n",
    "# Attach predictions to sliced dataframe\n",
    "df_slice[\"gpt4o_pred\"] = generated_answers\n",
    "\n",
    "# Save batch results (won‚Äôt break full CSV)\n",
    "output_file = OUTPUT_PATH.replace(\".csv\", f\"_batch_{START_INDEX}.csv\")\n",
    "df_slice.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"üíæ Saved batch results to {output_file}\")\n",
    "print(f\"‚úÖ Generated {len(df_slice)} predictions successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used gpt-4o-mini LLM-as-a-Judge \n",
    "def llm_as_judge(question, true_answer, pred_answer, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Use GPT-4o-mini to evaluate semantic similarity between true and predicted answers.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a financial QA evaluator. \n",
    "Compare the models predicted answer with the true answer and rate it 0.0 - 1.0 on:\n",
    "\n",
    "1. Correctness ‚Äî factual alignment with true answer.\n",
    "2. Completeness ‚Äî covers all key details.\n",
    "3. Relevance ‚Äî focuses on the question asked.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "True Answer: {true_answer}\n",
    "Predicted Answer: {pred_answer}\n",
    "\n",
    "Return JSON only:\n",
    "{{\"score\": 0.xx, \"justification\": \"short explanation\"}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        result = json.loads(text)\n",
    "        return result.get(\"score\", 0.0), result.get(\"justification\", \"\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Judge error:\", e)\n",
    "        return None, str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af3d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved at sample 10\n",
      "üíæ Progress saved at sample 20\n",
      "üíæ Progress saved at sample 30\n",
      "üíæ Progress saved at sample 40\n",
      "üíæ Progress saved at sample 50\n",
      "\n",
      "‚úÖ Completed evaluation ‚Üí Saved to /Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-LLM-Judge-Results.csv\n",
      "Average Score: 0.650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch Process Generation + Judging\n",
    "judge_model = \"gpt-4o-mini\"\n",
    "gen_model = \"gpt-4o\"\n",
    "\n",
    "scores, reasons, answers = [], [], []\n",
    "save_path = \"/Users/pulinkumar/Desktop/ALGOVERSE/Financial-Agent-LLM-Judge-Results.csv\"\n",
    "\n",
    "for i, row in tqdm(df.head(50).iterrows(), total=50, disable=True):\n",
    "    q, true_a = row[\"question\"], row[\"answer\"]\n",
    "\n",
    "    # Retrieve context and generate LLM answer\n",
    "    ctx = retrieve_contexts(q, top_k=1)[0]\n",
    "    pred = generate_answer_llm(q, context=ctx, model=gen_model)\n",
    "\n",
    "    # Evaluate with GPT-4o-mini\n",
    "    s, r = llm_as_judge(q, true_a, pred, model=judge_model)\n",
    "\n",
    "    answers.append(pred)\n",
    "    scores.append(s)\n",
    "    reasons.append(r)\n",
    "\n",
    "    # Simple rate-limit management\n",
    "    time.sleep(3)\n",
    "\n",
    "    #Fixed checkpoint save every 10 iterations\n",
    "    if (i + 1) % 10 == 0:\n",
    "        partial_df = pd.DataFrame({\n",
    "            \"question\": df.head(i + 1)[\"question\"].values,\n",
    "            \"true_answer\": df.head(i + 1)[\"answer\"].values,\n",
    "            \"pred_answer\": answers,\n",
    "            \"score\": scores,\n",
    "            \"reason\": reasons\n",
    "        })\n",
    "        partial_df.to_csv(save_path, index=False)\n",
    "        print(f\"üíæ Progress saved at sample {i+1}\")\n",
    "\n",
    "#Final Save\n",
    "results_df = pd.DataFrame({\n",
    "    \"question\": df.head(len(scores))[\"question\"].values,\n",
    "    \"true_answer\": df.head(len(scores))[\"answer\"].values,\n",
    "    \"pred_answer\": answers,\n",
    "    \"score\": scores,\n",
    "    \"reason\": reasons\n",
    "})\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(f\"\\n Completed evaluation ‚Üí Saved to {save_path}\")\n",
    "print(f\"Average Score: {np.nanmean([s for s in scores if s is not None]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155b665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
